# Methodology: Code-Switched Text Toxicity Analysis

## Abstract

This document presents a comprehensive methodology for analyzing toxicity patterns in code-switched English-Hindi text generated by large language models. Our approach combines computational linguistics techniques, machine learning models, and human evaluation to understand how language mixing affects toxicity detection and perception. The methodology encompasses data extraction, translation, code-switching generation, toxicity assessment, and human validation across multiple model architectures.

---

## 1. Data Collection and Preprocessing

### 1.1 Source Data Extraction

The initial data collection process begins with extracting prompts from the RTP-LX (Reasoning, Thinking, Planning - Language eXtended) dataset, which provides parallel English-Hindi text pairs suitable for code-switching research.

**Script: `0_extract_prompts.sh`**
```bash
#!/bin/bash
#SBATCH --partition=rome
#SBATCH --job-name=0_extract_prompts
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=02:00:00

python final_python_scripts/extract_prompts.py final_data/RTP-LX/RTP_LX_HI.json final_data/extracted_prompts/train_hi.txt
python final_python_scripts/extract_prompts.py final_data/RTP-LX/RTP_LX_EN.json final_data/extracted_prompts/train_en.txt
```

The extraction process involves parsing JSON-formatted datasets and converting them into plain text files containing individual prompts. The `extract_prompts.py` script handles the extraction logic, ensuring proper encoding and formatting for downstream processing. This step is crucial as it establishes the foundation dataset from which all subsequent code-switched variants will be generated.

### 1.2 Bidirectional Translation Pipeline

To create robust training data for code-switching generation, we implement a bidirectional translation approach using Google Translate API. This methodology ensures that we have high-quality translations in both directions, which are essential for creating meaningful word alignments.

**Script: `1_translate_file.sh`**
```bash
#!/bin/bash
#SBATCH --partition=rome
#SBATCH --job-name=1_translate_file
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=02:00:00

python final_python_scripts/translate_file.py \
    --input final_data/extracted_prompts/train_en.txt \
    --target hi \
    --output final_data/translate_api_outputs/train_hi.txt

python final_python_scripts/translate_file.py \
    --input final_data/extracted_prompts/train_hi.txt \
    --target en \
    --output final_data/translate_api_outputs/train_en.txt
```

The translation pipeline serves multiple purposes:
1. **Gold Standard Creation**: Original human translations serve as gold standard alignments
2. **Silver Standard Generation**: Machine translations provide alternative alignment sources
3. **Quality Validation**: Bidirectional translation enables consistency checking
4. **Alignment Diversity**: Multiple translation sources create varied alignment patterns

The `translate_file.py` script implements robust error handling, rate limiting for API calls, and quality validation to ensure translation reliability. This bidirectional approach is particularly important for Hindi-English pairs due to significant grammatical and syntactic differences between the languages.

---

## 2. Code-Switching Generation Framework

### 2.1 Alignment Generation Using MGIZA++

The core of our code-switching methodology relies on word-level alignments between English and Hindi text. We employ MGIZA++, a statistical word alignment tool, to create alignment mappings that guide the code-switching process.

**Script: `2_run_ezswitch_hi.sh` (Alignment Section)**
```bash
# Generate Alignment Files from gold translations
python ezswitch/alignment/giza-py/giza.py \
    --bin ezswitch/alignment/mgiza/mgizapp/bin \
    --source final_data/extracted_prompts/train_en.txt \
    --target final_data/extracted_prompts/train_hi.txt \
    --alignments final_data/alignments/en-hi_align_gold.txt

# Generate Alignment Files from silver translations
python ezswitch/alignment/giza-py/giza.py \
    --bin ezswitch/alignment/mgiza/mgizapp/bin \
    --source final_data/extracted_prompts/train_en.txt \
    --target final_data/translate_api_outputs/train_hi.txt \
    --alignments final_data/alignments/en-hi_align_silver.txt
```

The alignment generation process involves several sophisticated steps:

1. **Preprocessing**: Text normalization, tokenization, and cleaning
2. **Statistical Modeling**: MGIZA++ employs IBM Models 1-6 for alignment probability estimation
3. **Alignment Extraction**: Word-to-word and phrase-to-phrase alignment identification
4. **Quality Filtering**: Removal of low-confidence alignments

This alignment methodology creates two distinct alignment types:
- **Gold Alignments**: Based on human translations, representing high-quality semantic mappings
- **Silver Alignments**: Based on machine translations, providing additional alignment diversity

### 2.2 Multi-Model Code-Switching Generation

Our methodology employs three distinct large language models to generate code-switched text, each offering different strengths and characteristics:

**Models Used:**
1. **Aya-23-8B**: Multilingual model specifically designed for diverse language support
2. **Meta-Llama-3.1-8B-Instruct**: Latest instruction-tuned model with enhanced reasoning capabilities
3. **Meta-Llama-3-8B-Instruct**: Baseline Llama model for comparison

**Generation Process:**
```bash
python ezswitch/src/inference.py \
    --src final_data/extracted_prompts/train_en.txt \
    --tgt final_data/extracted_prompts/train_hi.txt \
    --src_translated final_data/translate_api_outputs/train_en.txt \
    --tgt_translated final_data/translate_api_outputs/train_hi.txt \
    --gold_align final_data/alignments/en-hi_align_gold.txt \
    --silver_src_align final_data/alignments/en-hi_align_silver.txt \
    --silver_tgt_align final_data/alignments/en-hi_align_silver.txt \
    --model_id "CohereForAI/aya-23-8B" \
    --output final_data/output/hindi/aya_23_8B.csv
```

**Generation Methods:**
1. **Baseline Method**: Standard generation without specific alignment constraints
2. **Silver Method**: Utilizes machine translation alignments for guidance
3. **Gold Method**: Employs human translation alignments for optimal quality

The EZSwitch framework implements a sophisticated algorithm that:
- Analyzes sentence structure and syntax
- Identifies optimal switching points based on alignments
- Maintains grammatical coherence across language boundaries
- Preserves semantic meaning throughout the switching process

### 2.3 Compilation and Integration

Following individual model generation, we compile all outputs into a unified dataset structure:

```bash
python ezswitch/src/compile.py \
    --directory final_data/output/hindi \
    --output final_data/output/compile_hindi.csv
```

This compilation process creates a comprehensive dataset containing:
- Source English sentences
- Target Hindi sentences
- Code-switched variants from all models
- Generation metadata (method, model, parameters)
- Quality metrics and validation flags

---

## 3. Post-Processing and Quality Control

### 3.1 Primary Key Assignment and Data Organization

**Script: `3_processing_ezswitch_generation.sh`**

The post-processing pipeline begins with systematic data organization and quality control measures:

```bash
python final_python_scripts/add_primary_keys.py \
    --input final_data/output/compile_hindi.csv \
    --output final_outputs/filtered_output.csv \
    --key_prefix "cs_"
```

Primary key assignment serves multiple critical functions:
1. **Unique Identification**: Each generated sentence receives a unique identifier (cs_XXXXXX)
2. **Traceability**: Links generated text back to source data and generation parameters
3. **Quality Control**: Enables systematic tracking throughout the analysis pipeline
4. **Reproducibility**: Facilitates replication and validation of specific results

### 3.2 Language Detection and Analysis

Following primary key assignment, we implement comprehensive language detection using IndicLID, a state-of-the-art language identification system specifically designed for Indian languages:

```bash
python final_python_scripts/language_detection.py \
    --input_file final_outputs/filtered_output.csv \
    --output_file final_outputs/filtered_output.csv
```

The language detection process provides detailed linguistic analysis:

**Metrics Computed:**
- `hindi_word_count`: Number of words identified as Hindi (Devanagari script)
- `english_word_count`: Number of words identified as English
- `romanized_hindi_count`: Hindi words written in Roman script
- `total_hindi_count`: Combined Hindi words (Devanagari + Romanized)
- `total_words`: Total word count in the sentence
- `hindi_percent`: Percentage of Hindi words
- `english_percent`: Percentage of English words
- `romanized_hindi_percent`: Percentage of romanized Hindi
- `total_hindi_percent`: Total Hindi percentage (all scripts)

This granular analysis enables sophisticated understanding of code-switching patterns and linguistic behavior across different models and generation methods.

### 3.3 Language Mix Filtering

The final post-processing step implements intelligent filtering to ensure high-quality code-switched examples:

```bash
python final_python_scripts/filter_language_mix.py \
    --input final_outputs/filtered_output.csv \
    --output final_outputs/filtered_output.csv
```

**Filtering Criteria:**
1. **Minimum Language Mixing**: Sentences must contain both English and Hindi elements
2. **Balance Requirements**: Neither language should dominate completely (e.g., >95%)
3. **Coherence Validation**: Generated text must maintain syntactic coherence
4. **Length Constraints**: Appropriate sentence length for meaningful analysis

This filtering ensures that the final dataset contains authentic code-switching examples rather than predominantly monolingual text with minimal mixing.

---

## 4. Model Continuation Generation

### 4.1 Multi-Model Continuation Framework

**Script: `4_generate_continuations.sh`**

The continuation generation phase represents a critical component of our toxicity analysis methodology. We generate continuations using three different models to understand how different architectures handle code-switched input:

```bash
python final_python_scripts/generate_continuations_local.py \
    --input final_outputs/filtered_output.csv \
    --output final_outputs/continuations.csv \
    --token "$HUGGINGFACE_API_KEY" \
    --max_tokens 50 \
    --temperature 0.7 \
    --batch_size 16
```

**Generation Parameters:**
- **Max Tokens**: 50 tokens to ensure substantial continuation while maintaining focus
- **Temperature**: 0.7 for balanced creativity and coherence
- **Batch Size**: 16 for optimal GPU utilization and processing efficiency

**Continuation Types Generated:**
1. **Source Continuations**: Using original English text as prompt
2. **Target Continuations**: Using original Hindi text as prompt
3. **Generated Continuations**: Using code-switched text as prompt

This multi-prompt approach enables comparative analysis of how different input languages affect model behavior and toxicity patterns.

### 4.2 Technical Implementation Details

The continuation generation system implements several sophisticated features:

**Memory Management:**
- Efficient batching to prevent GPU memory overflow
- Dynamic batch size adjustment based on input length
- Garbage collection between batches

**Quality Control:**
- Input validation and sanitization
- Output filtering for malformed generations
- Error handling and retry mechanisms

**Reproducibility:**
- Seed management for consistent results
- Parameter logging for all generations
- Comprehensive metadata capture

---

## 5. Toxicity Assessment Pipeline

### 5.1 Comprehensive Perspective API Analysis

**Script: `5_run_perspective_api.sh`**

The toxicity assessment phase employs Google's Perspective API to analyze multiple dimensions of toxicity across all text variants:

```bash
python final_python_scripts/run_perspective_api.py \
    --input final_outputs/continuations.csv \
    --output final_outputs/perspective_analysis.csv \
    --api_key "$PERSPECTIVE_API_KEY" \
    --batch_size 10 \
    --rate_limit 60
```

**Toxicity Dimensions Analyzed:**
1. **TOXICITY**: General toxicity assessment
2. **SEVERE_TOXICITY**: Detection of severe toxic content
3. **IDENTITY_ATTACK**: Personal or group-based attacks
4. **INSULT**: Insulting or offensive language
5. **PROFANITY**: Profane or vulgar language
6. **THREAT**: Threatening language or behavior

**Text Types Analyzed:**
- Original source text (English)
- Original target text (Hindi)
- Generated code-switched text
- All model continuations (3 models × 3 text types = 9 continuation variants)

This comprehensive analysis creates a dataset with 95 columns, providing unprecedented detail for toxicity analysis in multilingual contexts.

### 5.2 Rate Limiting and API Management

The Perspective API implementation includes sophisticated rate limiting and error handling:

**Rate Limiting Strategy:**
- Maximum 60 requests per minute to comply with API limits
- Exponential backoff for rate limit violations
- Intelligent queuing system for large datasets

**Error Handling:**
- Retry mechanisms for transient failures
- Graceful degradation for API unavailability
- Comprehensive logging for debugging and monitoring

**Data Validation:**
- Input sanitization to prevent API errors
- Output validation for score ranges and formats
- Missing data handling for incomplete responses

---

## 6. Additional Datasets and Comparative Analysis

### 6.1 SemEval-2020 Task 9 Tweets Dataset

#### 6.1.1 Dataset Overview

To validate and extend our code-switching toxicity analysis, we incorporate the SemEval-2020 Task 9 Hinglish dataset, which contains authentic code-switched social media content. This dataset provides real-world code-switched text from Twitter, enabling comparison between artificially generated and naturally occurring code-switched content.

**Source Location**: `data/Semeval_2020_task9_data/Hinglish/Hinglish_train_14k_split_conll.txt`

**Dataset Characteristics**:
- Format: CoNLL-style annotation with word-level language labels
- Size: 14,000 training examples
- Languages: English-Hindi code-switched text
- Domain: Social media (Twitter)
- Annotation: Word-level language identification (Eng/Hin)
- Sentiment: Labeled sentiment annotations

#### 6.1.2 Processing Pipeline

The SemEval tweets dataset undergoes a specialized 6-step processing pipeline optimized for social media content analysis:

**Step 1: Dataset Extraction and Processing**

**Script: `tweets_job_scripts/01_get_dataset.sh`**
```bash
#!/bin/bash
#SBATCH --partition=rome
#SBATCH --job-name=01_get_dataset
#SBATCH --mem=64G
#SBATCH --time=48:00:00

python tweets_python_scripts/process_hinglish.py
```

The `process_hinglish.py` script converts the CoNLL format into a structured CSV format:

```python
# Key Processing Steps:
1. Parse CoNLL format (word\tlanguage_tag pairs)
2. Extract metadata (primary_key, sentiment)
3. Calculate language composition metrics:
   - Hindi word count (romanized)
   - English word count
   - Total words and percentages
4. Reconstruct full sentences from word annotations
5. Generate comprehensive language statistics
```

**Output Structure**:
```csv
generated,primary_key,sentiment,hindi_word_count,english_word_count,
romanized_hindi_count,total_hindi_count,total_words,hindi_percent,
romanized_hindi_percent,total_hindi_percent,english_percent
```

**Step 2: Multi-Model Continuation Generation**

**Script: `tweets_job_scripts/02_generate_continuations.sh`**
```bash
#!/bin/bash
#SBATCH --partition=gpu_a100
#SBATCH --gpus=4
#SBATCH --time=24:00:00

# Process with Llama 3
python new_python_scripts/generate_continuations_local.py \
    --input tweets_outputs/processed_hinglish.csv \
    --output tweets_outputs/llama_local/code_switched_llama3.csv \
    --model meta-llama/Meta-Llama-3-8B-Instruct \
    --text_column generated \
    --max_tokens 50 \
    --temperature 0.7

# Process with Llama 3.1
python new_python_scripts/generate_continuations_local.py \
    --input tweets_outputs/processed_hinglish.csv \
    --output tweets_outputs/llama_local/code_switched_llama31.csv \
    --model meta-llama/Meta-Llama-3.1-8B-Instruct \
    --text_column generated \
    --max_tokens 50 \
    --temperature 0.7

# Process with Aya
python new_python_scripts/generate_continuations_local.py \
    --input tweets_outputs/processed_hinglish.csv \
    --output tweets_outputs/llama_local/code_switched_aya.csv \
    --model CohereForAI/aya-23-8B \
    --text_column generated \
    --max_tokens 50 \
    --temperature 0.7
```

**Step 3: Toxicity Assessment**

**Script: `tweets_job_scripts/03_run_perspective.sh`**
```bash
#!/bin/bash
#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --time=01:00:00

python new_python_scripts/run_perspective.py \
    --input_file tweets_outputs/llama_local/code_switched_aya.csv \
    --output_file tweets_outputs/perspective/aya_continuations_perspective.csv \
    --text_column continuation \
    --batch_size 100
```

**Step 4: Perplexity Analysis**

**Script: `tweets_job_scripts/04_perplexity_analysis.sh`**
```bash
#!/bin/bash
#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --time=01:00:00

# Analyze perplexity for each model
python new_python_scripts/analyze_perplexity_language_toxicity.py \
    --input_csv tweets_outputs/perspective/llama3_continuations_perspective.csv \
    --output_csv tweets_outputs/analysis/llama3_perplexity_analysis.csv \
    --model meta-llama/Meta-Llama-3-8B-Instruct

python new_python_scripts/analyze_perplexity_language_toxicity.py \
    --input_csv tweets_outputs/perspective/llama31_continuations_perspective.csv \
    --output_csv tweets_outputs/analysis/llama31_perplexity_analysis.csv \
    --model meta-llama/Meta-Llama-3.1-8B-Instruct

python new_python_scripts/analyze_perplexity_language_toxicity.py \
    --input_csv tweets_outputs/perspective/aya_continuations_perspective.csv \
    --output_csv tweets_outputs/analysis/aya_perplexity_analysis.csv \
    --model CohereForAI/aya-23-8B
```

**Step 5: Correlation Analysis**

**Script: `tweets_job_scripts/05_toxicity_perplexity_correlation.sh`**
```bash
#!/bin/bash
#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --time=01:00:00

python new_python_scripts/perplexity_toxicity_correlation.py \
    --llama3_csv tweets_outputs/analysis/llama3_perplexity_analysis.csv \
    --llama31_csv tweets_outputs/analysis/llama31_perplexity_analysis.csv \
    --aya_csv tweets_outputs/analysis/aya_perplexity_analysis.csv \
    --output_dir tweets_outputs/analysis
```

**Step 6: Comprehensive Exploratory Data Analysis**

**Script: `tweets_job_scripts/06_exploratory_data_analysis.sh`**
```bash
#!/bin/bash
#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --time=01:00:00

python new_python_scripts/exploratory_data_analysis.py \
    --input_csv tweets_outputs/processed_hinglish.csv \
    --output_dir tweets_outputs/analysis
```

#### 6.1.3 Specialized Analysis Components

**A. Language Composition Analysis**

The tweets dataset analysis includes sophisticated language composition metrics:

```python
# Language Distribution Analysis
- Hindi percentage distribution across models
- English percentage distribution across models
- Word count statistics and distributions
- Language mixing patterns in social media text
- Sentiment correlation with language choice
```

**B. Toxicity-Perplexity Correlation Analysis**

The `perplexity_toxicity_correlation.py` script performs comprehensive correlation analysis:

```python
# Correlation Analysis Features:
1. Cross-model perplexity comparison
2. Outlier impact analysis (IQR, Z-score, percentile methods)
3. Statistical significance testing
4. Correlation heatmaps with significance markers
5. Model-specific scatter plots and trend analysis
6. Robust correlation estimation with multiple methods
```

**C. Exploratory Data Analysis (EDA)**

The comprehensive EDA framework (`exploratory_data_analysis.py`) provides:

```python
# EDA Components:
1. Basic dataset statistics and summaries
2. Language composition analysis across models
3. Sentiment distribution analysis
4. Toxicity metrics assessment
5. Text characteristics analysis
6. Generated text language distribution
7. Perspective API score distributions
8. Comprehensive correlation analysis
9. Summary report generation
```

**Key EDA Features**:
- **Multi-model comparison**: Analyzes differences across Llama3, Llama3.1, and Aya models
- **Language mixing patterns**: Identifies trends in code-switching behavior
- **Toxicity distribution**: Examines toxicity patterns in real social media content
- **Quality assessment**: Validates model performance on authentic data

#### 6.1.4 Outputs and Results

The SemEval tweets analysis generates comprehensive outputs:

**Primary Output Files**:
- `tweets_outputs/processed_hinglish.csv`: Processed dataset with language metrics
- `tweets_outputs/analysis/`: Model-specific perplexity and correlation results
- `tweets_outputs/perspective/`: Toxicity scores for all model continuations
- `tweets_outputs/eda_analysis/`: Comprehensive visualizations and statistics

**Key Insights Derived**:
1. **Real vs. Artificial Code-switching**: Comparison between generated and authentic content
2. **Social Media Language Patterns**: Understanding natural code-switching behavior
3. **Model Performance on Authentic Data**: Validation of generation quality
4. **Toxicity in Natural Code-switching**: Baseline toxicity levels in social media

### 6.2 HInge Dataset for Baseline Comparison

#### 6.2.1 Dataset Overview

To establish robust baseline comparisons for perplexity analysis, we incorporate the HInge (Hindi-English) dataset, which provides high-quality monolingual Hindi content. This dataset serves as a critical control for understanding whether our models exhibit systematic bias against code-switched content compared to monolingual text.

**Source Location**: `ezswitch/data/hinge/train_human_generated.pkl`
**Converted Format**: `ezswitch/data/hinge/train_human_generated.csv`

**Dataset Characteristics**:
- Content: Human-generated monolingual Hindi text
- Quality: High-quality, grammatically correct Hindi sentences
- Domain: Diverse topics and contexts
- Purpose: Baseline perplexity measurement for monolingual content

#### 6.2.2 Data Conversion and Processing

**Conversion Script: `convert_pkl_to_csv.py`**
```python
import pickle
import pandas as pd

# Read the pickle file
with open('ezswitch/data/hinge/train_human_generated.pkl', 'rb') as f:
    data = pickle.load(f)

# Convert dictionary to DataFrame
df = pd.DataFrame.from_dict(data, orient='index')

# Save as CSV for easier processing
output_path = 'ezswitch/data/hinge/train_human_generated.csv'
df.to_csv(output_path, index=True)
```

#### 6.2.3 Perplexity Comparison Analysis

The HInge dataset undergoes comprehensive perplexity analysis to establish baseline measurements for monolingual content. This analysis is crucial for determining whether language models exhibit inherent bias against code-switched text.

**Analysis Results Location**: `python_scripts/output/perplexity_hinge/`

**Output Files**:
- `perplexity_results.csv`: Detailed perplexity measurements
- `perplexity_comparison.png`: Visualization of perplexity distributions

#### 6.2.4 Perplexity Results Analysis

The HInge perplexity analysis reveals significant patterns in model behavior:

**Sample Results** (from `perplexity_results.csv`):
```csv
model,language,perplexity
llama-3.1,English,24.01,5.90,222.04,151.52,488.36,...
llama-3.1,Hindi,9.76,6.92,7.00,25.10,4.80,...
```

**Key Findings**:

1. **Language-Specific Perplexity Patterns**:
   - Hindi sentences: Mean perplexity ≈ 15-20 (lower values indicate better model familiarity)
   - English sentences: Mean perplexity ≈ 100-150 (higher variance observed)
   - Clear distinction between monolingual perplexity distributions

2. **Model Behavior Analysis**:
   - Llama-3.1 shows consistent performance across both languages
   - Lower perplexity values for Hindi suggest better model adaptation
   - Significant variance in English perplexity indicates context dependency

3. **Baseline Establishment**:
   - Provides reference distribution for monolingual content
   - Enables statistical comparison with code-switched perplexity
   - Establishes threshold values for normal vs. abnormal perplexity

#### 6.2.5 Integration with Main Analysis

The HInge baseline comparison integrates with the main research objectives through:

**Research Point 4 Implementation**:
- Statistical comparison of perplexity distributions
- Effect size calculation (Cohen's d) between monolingual and code-switched content
- Hypothesis testing for systematic bias detection

**Comparison Methodology**:
```python
# Statistical Testing Framework
from scipy.stats import ttest_ind, mannwhitneyu

# Compare perplexity distributions
hinge_perplexity = load_hinge_perplexity()
cs_perplexity = load_codeswitch_perplexity()

# T-test for distribution comparison
t_stat, p_value = ttest_ind(hinge_perplexity, cs_perplexity)

# Mann-Whitney U test for non-parametric comparison
u_stat, u_p_value = mannwhitneyu(hinge_perplexity, cs_perplexity)

# Effect size calculation
cohen_d = calculate_effect_size(hinge_perplexity, cs_perplexity)
```

**Expected Insights**:
1. **Bias Detection**: Quantifies whether models assign higher perplexity to code-switched content
2. **Model Comparison**: Evaluates which models show least bias against multilingual content
3. **Threshold Setting**: Establishes reference points for perplexity-based quality assessment
4. **Validation**: Confirms whether observed patterns in code-switched text are statistically significant

### 6.3 Comparative Analysis Framework

#### 6.3.1 Cross-Dataset Validation

The integration of SemEval tweets and HInge datasets creates a comprehensive validation framework:

**Validation Dimensions**:
1. **Artificial vs. Natural**: Generated code-switching vs. authentic social media content
2. **Monolingual vs. Multilingual**: Pure Hindi (HInge) vs. code-switched content
3. **Domain Variation**: Formal prompts vs. social media vs. conversational text
4. **Quality Assessment**: Machine-generated vs. human-created content

#### 6.3.2 Statistical Robustness

The multi-dataset approach provides statistical robustness through:

**Sample Size Enhancement**:
- Main dataset: ~4,000 generated examples
- SemEval tweets: ~14,000 authentic examples  
- HInge dataset: Additional monolingual baseline

**Diversity Validation**:
- Multiple domains (prompts, social media, conversational)
- Various generation methods (EZSwitch, direct generation)
- Different quality levels (human vs. machine generated)

**Cross-Validation Opportunities**:
- Model consistency across datasets
- Toxicity pattern replication
- Perplexity behavior validation
- Human evaluation correlation

---

## 7. Human Evaluation Methodology

### 7.1 Google Form Design and Implementation

**Script: `temp_scripts/create_google_form.py`**

The human evaluation component represents a crucial validation step for our automated toxicity detection. We developed a systematic approach to create and deploy human evaluation surveys using the `create_google_form.py` script, which automates the form creation process and ensures consistent question formatting across evaluations.

**Form Structure:**
1. **Participant Background**: Language proficiency, demographics, code-switching familiarity
2. **Question 1**: "How natural does the code-switched text sound?" [1-5 scale: Not natural → Natural]
3. **Question 2**: "How toxic is this code-switched text?" [1-5 scale: Not toxic → Toxic]
4. **Question 3**: "Compared to the original English version (src column), is this code-switched version:" [Less toxic / Similarly toxic / More toxic / Cannot be determined]
5. **Question 4**: "Which words/phrases are the most toxic? (Please specify if they are in Hindi, English, or both)" [Short text answer]

**Why This Design Is Excellent:**
- **Naturalness-Perplexity Link**: Q1 directly tests whether human naturalness judgments correlate with model perplexity scores
- **Perspective API Validation**: Q2 provides direct comparison with automated toxicity detection
- **Content-Controlled Comparison**: Q3 brilliantly controls for semantic content while testing linguistic form effects
- **Language-Specific Analysis**: Q4 enables fine-grained analysis of which language carries toxicity burden
- **Multiple Research Questions**: Single form addresses naturalness, toxicity validation, comparative assessment, and language-specific effects
- **Efficiency**: Compact design reduces participant fatigue while maximizing data quality

### 7.2 Random Sampling Strategy

**Script: `temp_scripts/sample_random_data.py`**

To ensure unbiased evaluation and comprehensive model representation, we implemented a sophisticated random sampling strategy using the `sample_random_data.py` script. This script ensures that the Google Form evaluation includes diverse examples without being fixated on code-switched sentences generated by only one model.

**Sampling Implementation:**
```python
def stratified_random_sampling():
    # Load the complete dataset
    df = pd.read_csv('final_outputs/perspective_analysis.csv')
    
    # Ensure representation across all models
    models = ['llama_3_8B', 'llama_3_1_8B', 'aya_23_8B']
    methods = ['baseline', 'silver', 'gold']
    
    # Stratified sampling by model and method
    sampled_data = []
    for model in models:
        for method in methods:
            subset = df[(df['model'] == model) & (df['method'] == method)]
            sample = subset.sample(n=min(5, len(subset)), random_state=42)
            sampled_data.append(sample)
    
    return pd.concat(sampled_data, ignore_index=True)
```

**Sampling Criteria:**
1. **Model Balance**: Equal representation from Aya-23-8B, Llama-3-8B, and Llama-3.1-8B
2. **Method Distribution**: Balanced sampling across baseline, silver, and gold generation methods
3. **Toxicity Range**: Inclusion of various toxicity levels from Perspective API scores
4. **Linguistic Diversity**: Different code-switching patterns and language ratios
5. **Quality Control**: Exclusion of malformed or extremely short/long sentences

**Statistical Considerations:**
- Target sample size: 50-100 unique sentences for statistical power
- Random seed management for reproducibility
- Stratification to ensure representative coverage
- Balance between diversity and manageability for human evaluators

### 7.3 Data Quality and Validation

The human evaluation methodology incorporates multiple quality control measures:

**Participant Screening:**
- Language proficiency requirements (bilingual English-Hindi speakers)
- Familiarity with code-switching practices in natural conversation
- Basic demographic information for potential bias analysis

**Response Validation:**
- Attention check questions embedded within the survey
- Consistency verification across related assessment items
- Completion time monitoring to ensure thoughtful engagement

**Inter-Rater Reliability Planning:**
- Multiple evaluators for a subset of sentences
- Krippendorff's alpha calculation for agreement measurement
- Outlier detection and systematic disagreement analysis

---

## 8. Future Analysis Pipeline

### 8.1 Perplexity Analysis Framework

Our methodology includes comprehensive perplexity analysis using multiple models and approaches. The following scripts represent planned extensions to our current analysis:

**Script: `new_job_scripts/15_perplexity.sh`**
This script implements perplexity calculation across multiple model architectures to understand model confidence in processing code-switched versus monolingual text. The analysis will employ models like mT5-XL, multilingual BERT, and the original generation models (Aya, Llama-3, Llama-3.1) to calculate perplexity scores for:
- Original English source text
- Original Hindi target text  
- Generated code-switched text
- Model continuations

**Script: `new_job_scripts/16_rtplx_toxicity_compare.sh`**
This comparative analysis script will investigate the relationship between model perplexity and toxicity scores, addressing key research questions:
- Do models assign higher perplexity to more toxic content?
- Is there a correlation between model uncertainty (high perplexity) and toxicity detection accuracy?
- How does perplexity-toxicity correlation vary across different models and languages?

**Script: `new_job_scripts/17_perplexity_check.sh`**
Quality validation and consistency verification for perplexity calculations, including:
- Cross-model perplexity comparison for validation
- Outlier detection and analysis for unusual perplexity scores
- Consistency checks between different perplexity calculation methods

### 8.2 Comparative Analysis Extensions

**Script: `new_job_scripts/18_tox_perplexity_in_out_compare.sh`**
This script analyzes the relationship between input text characteristics and generated continuation properties:
- Input toxicity vs. output toxicity correlation
- Input perplexity vs. output perplexity relationships
- Model-specific patterns in toxicity amplification or reduction

**Script: `new_job_scripts/19_run_perplexity_on_srctgt_full_gen.sh`**
Comprehensive perplexity analysis across all text variants:
- Source text perplexity using various models
- Target text perplexity across different model architectures
- Generated text perplexity comparison with source and target baselines
- Cross-linguistic perplexity analysis for bilingual understanding

**Script: `new_job_scripts/20_relative_input_output.sh`**
Relative comparison analysis between input characteristics and output properties:
- Delta analysis for toxicity changes from input to continuation
- Perplexity shift analysis from prompt to generated content
- Language mixing ratio changes in continuations versus inputs

**Script: `new_job_scripts/21_add_perspective_generated.sh`**
Extended Perspective API analysis for additional content and validation:
- Re-analysis of generated content with updated API parameters
- Additional toxicity dimensions if available
- Validation runs for consistency checking

---

## 9. Technical Infrastructure and Computational Resources

### 9.1 High-Performance Computing Environment

Our methodology leverages the Snellius supercomputing cluster, which provides the computational resources necessary for large-scale multilingual NLP analysis:

**Hardware Specifications:**
- **GPU Nodes**: NVIDIA A100 (40GB/80GB) for model inference and training tasks
- **CPU Nodes**: AMD Rome processors (128 cores) for general computation and preprocessing
- **Memory**: 256GB-1TB RAM depending on task requirements
- **Storage**: High-speed parallel file system with multi-TB capacity for dataset storage
- **Network**: High-bandwidth InfiniBand for efficient data transfer

**Resource Allocation Strategy:**
```bash
#SBATCH --partition=gpu_a100     # For model inference tasks
#SBATCH --partition=rome         # For CPU-intensive preprocessing
#SBATCH --gpus=1                 # Single GPU for most inference tasks
#SBATCH --cpus-per-task=18       # Optimal CPU allocation for preprocessing
#SBATCH --mem=64G                # Memory allocation based on task requirements
#SBATCH --time=02:00:00          # Conservative time estimates with safety margins
```

### 9.2 Software Environment and Dependencies

**Environment Management:**
```bash
module load 2023 Miniconda3/23.5.2-0
conda create -n code-switch python=3.9
conda activate code-switch
```

**Key Dependencies:**
- **PyTorch** (≥1.12.0): For model loading, inference, and tensor operations
- **Transformers** (≥4.21.0): Hugging Face library for model implementations
- **Pandas** (≥1.5.0): Data manipulation and analysis
- **NumPy** (≥1.21.0): Numerical computing foundation
- **SciPy** (≥1.8.0): Statistical analysis and scientific computing
- **Matplotlib/Seaborn**: Data visualization and plotting
- **IndicLID**: Specialized language identification for Indian languages
- **Google Cloud Libraries**: For Perspective API and Translation API access

### 9.3 Reproducibility and Version Control

**Environment Reproducibility:**
- Conda environment YAML files for exact dependency reproduction
- Docker containerization for cross-platform compatibility
- Comprehensive requirements.txt with pinned versions

**Code Version Control:**
- Git repositories for all scripts and configuration files
- Systematic branching for experimental variations
- Tag-based releases for major methodology versions

**Data Management:**
- MD5 checksums for all input and intermediate datasets
- Systematic backup procedures for critical results
- Comprehensive logging for all processing steps and parameters

---

## 10. Quality Assurance and Validation

### 10.1 Data Validation Procedures

Throughout the methodology, we implement comprehensive quality assurance measures to ensure data integrity and result reliability:

**Input Validation:**
- Character encoding verification (UTF-8 compliance for multilingual content)
- Language detection accuracy validation using multiple detection systems
- Sentence boundary detection verification for proper text segmentation
- Duplicate detection and removal procedures

**Processing Validation:**
- Intermediate result checksum verification at each pipeline stage
- Statistical consistency checks for expected value ranges
- Format validation for CSV files and data structure compliance
- Cross-reference validation between related datasets

**Output Validation:**
- Range checking for all numerical outputs (toxicity scores, perplexity values)
- Completeness verification for all required data fields
- Consistency checks across different analysis approaches
- Outlier detection and investigation procedures

### 10.2 Error Handling and Recovery

**Robust Error Management:**
- Graceful degradation for API failures and rate limiting
- Automatic retry mechanisms with exponential backoff
- Comprehensive error logging with detailed diagnostics
- Fallback procedures for critical service unavailability

**Recovery Procedures:**
- Checkpointing for long-running processes to enable restart capability
- Partial result recovery for interrupted computations
- Manual intervention protocols for critical failure scenarios
- Data corruption detection and recovery mechanisms

**Monitoring and Alerting:**
- Real-time progress monitoring for all batch processes
- Resource utilization tracking to prevent system overload
- Automated alerts for anomalous results or processing failures
- Performance metrics collection for optimization

---

## 11. Ethical Considerations and Limitations

### 11.1 Ethical Framework

Our methodology incorporates several important ethical considerations:

**Data Privacy and Security:**
- Anonymization of all potentially identifiable information
- Secure handling and storage of sensitive content
- Compliance with GDPR and other relevant data protection regulations
- Clear data retention and deletion policies

**Content Responsibility:**
- Appropriate content warnings for human evaluators
- Clear labeling and handling of potentially offensive material
- Responsible disclosure of methodology limitations and potential misuse
- Ethical approval for human subject research components

**Bias Mitigation:**
- Diverse model selection to reduce single-model algorithmic bias
- Multiple evaluation approaches for methodological triangulation
- Cultural sensitivity in toxicity assessment and interpretation
- Transparent reporting of limitations and potential biases

### 11.2 Methodological Limitations

**Technical Limitations:**
- API rate limits affecting processing speed and scalability
- Model availability and access constraints for certain architectures
- Computational resource limitations for extremely large-scale analysis
- Language detection accuracy limitations for highly mixed content

**Linguistic Limitations:**
- Primary focus on English-Hindi language pair only
- Regional dialect and cultural variations not comprehensively covered
- Script mixing complexity in romanized versus native script usage
- Limited coverage of informal register and internet slang

**Evaluation Limitations:**
- Human evaluator availability and bilingual expertise requirements
- Subjective nature of toxicity and naturalness judgments
- Cultural and contextual factors affecting toxicity perception
- Limited sample size for human evaluation due to resource constraints

---

## 12. Conclusion and Future Directions

### 12.1 Methodological Contributions

This comprehensive methodology represents several significant contributions to the field of multilingual NLP and AI safety:

**Novel Analytical Framework:**
- First systematic approach to toxicity analysis in automatically generated code-switched text
- Multi-model comparative analysis revealing model-specific biases and behaviors
- Integration of computational metrics with human evaluation for robust validation

**Technical Innovations:**
- Scalable pipeline for large-scale code-switching generation and analysis
- Sophisticated quality control and validation procedures for multilingual content
- Comprehensive toxicity assessment across multiple dimensions and text types

**Methodological Rigor:**
- Systematic experimental design with appropriate controls and comparisons
- Robust statistical analysis framework for correlation and significance testing
- Comprehensive documentation enabling reproducibility and extension

### 12.2 Future Extensions and Applications

The methodology framework supports several important future directions:

**Cross-Linguistic Extensions:**
- Adaptation to other language pairs (English-Spanish, English-Arabic, English-Chinese)
- Investigation of language family effects on code-switching and toxicity patterns
- Cultural adaptation of toxicity definitions and assessment criteria

**Scalability Improvements:**
- Enhanced parallel processing for larger dataset analysis
- Optimization for distributed computing environments
- Integration with streaming data sources for real-time analysis

**Application Domains:**
- Social media content moderation system enhancement
- Educational technology safety for multilingual learning environments
- Conversational AI safety improvements for global deployment
- Policy development for multilingual AI governance and regulation

This methodology establishes a robust foundation for understanding toxicity in code-switched content while providing practical tools for building safer multilingual AI systems. The systematic approach ensures both scientific rigor and practical applicability across diverse research and industry contexts. 